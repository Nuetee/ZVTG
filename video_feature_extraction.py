import json
import cv2
import torch
import numpy as np
from PIL import Image
import os
import argparse
import sys
import subprocess

current_dir = os.path.dirname(os.path.abspath(__file__))
t2v_metrics_path = os.path.abspath(os.path.join(current_dir, "../t2v-feature-input"))

# sys.pathì— ì¶”ê°€í•˜ì—¬ import ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
sys.path.append(t2v_metrics_path)

from custom_clip_vision_encoder import CustomCLIPVisionEncoder  # âœ… Custom Vision Tower ì‚¬ìš©

# âœ… ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì • (í•œ ë²ˆë§Œ ë¡œë“œë¨)
vision_tower = None  
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_model():
    """âœ… ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    print("ğŸ”¹ CustomCLIPVisionEncoder ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•©ë‹ˆë‹¤...")
    model = CustomCLIPVisionEncoder(
        model_name="openai/clip-vit-large-patch14-336", 
        select_layer=-2, 
        select_feature="patch"
    ).to(device)
    return model

def extract_frame_ffmpeg(video_path, frame_index, output_path):
    """FFmpegë¥¼ ì´ìš©í•´ íŠ¹ì • í”„ë ˆì„ì„ ê°•ì œë¡œ ì¶”ì¶œ"""
    try:
        cmd = [
            "ffmpeg", "-i", video_path,
            "-vf", f"select=eq(n\\,{frame_index})",
            "-vsync", "vfr",
            output_path
        ]
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        return os.path.exists(output_path)
    except Exception as e:
        print(f"âš ï¸ FFmpeg extraction failed: {e}")
        return False

def frame_feature_extraction(dataset, vid, frame_idx, num_frames):
    global vision_tower  # âœ… ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©

    if vision_tower is None:
        vision_tower = load_model()  # âœ… ì²« ë²ˆì§¸ í˜¸ì¶œì—ì„œë§Œ ëª¨ë¸ ë¡œë“œë¨
    # else:
    #     print("ğŸ”¹ ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš© ì¤‘...")  # âœ… ë°˜ë³µ í˜¸ì¶œì—ì„œë„ ê°™ì€ ëª¨ë¸ ì‚¬ìš©

    # âœ… Image Processor ê°€ì ¸ì˜¤ê¸° (ì „ì—­ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜´)
    image_processor = vision_tower.image_processor

    # JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    with open(f'dataset/{dataset}/llm_outputs.json', 'r') as f:
        video_json = json.load(f)

    # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    if dataset == 'charades-sta':
        data_path = '../PRVR/video_data/Charades_v1/'
    elif dataset == 'activitynet':
        data_path = ['../PRVR/video_data/Activitynet_1-2/', '../PRVR/video_data/Activitynet_1-3/']
    else:
        raise ValueError("Unsupported dataset. Choose from 'charades-sta' or 'activitynet'.")

    # ë¹„ë””ì˜¤ keyë¡œ íŒŒì¼ ì°¾ê¸°
    matched_files = []
    if isinstance(data_path, list):  # ActivityNetì˜ ê²½ìš° ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ì°¾ê¸°
        for path in data_path:
            for file in os.listdir(path):
                if file.startswith(vid):
                    matched_files.append(os.path.join(path, file))
    else:  # Charades-staì˜ ê²½ìš° ë‹¨ì¼ ê²½ë¡œ
        for file in os.listdir(data_path):
            if file.startswith(vid):
                matched_files.append(os.path.join(data_path, file))

    if not matched_files:
        raise FileNotFoundError(f"No video file found for key: {vid}")

    video_path = matched_files[0]  # ì²« ë²ˆì§¸ ë§¤ì¹­ëœ íŒŒì¼ ì‚¬ìš©

    # âœ… `expand2square()` í•¨ìˆ˜ (ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°)
    def expand2square(pil_img, background_color):
        width, height = pil_img.size
        if width == height: 
            return pil_img
        elif width > height:
            result = Image.new(pil_img.mode, (width, width), background_color)
            result.paste(pil_img, (0, (width - height) // 2))
            return result
        else:
            result = Image.new(pil_img.mode, (height, height), background_color)
            result.paste(pil_img, ((height - width) // 2, 0))
            return result

    # ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video file: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    target_frame = int((frame_idx / num_frames) * total_frames)
    target_frame = min(target_frame, total_frames - 1)
    if target_frame >= total_frames:
        raise ValueError(f"Requested frame index {target_frame} exceeds total frames {target_frame}.")

    # íŠ¹ì • í”„ë ˆì„ìœ¼ë¡œ ì´ë™
    cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
    ret, frame = cap.read()
    cap.release()

    if not ret or frame is None:
        print(f"âš ï¸ OpenCV failed to read frame {target_frame}. Trying FFmpeg...")
        frame_save_path = f"temp_frame_{vid}_{target_frame}.jpg"
        if not extract_frame_ffmpeg(video_path, target_frame, frame_save_path):
            print(f"âš ï¸ FFmpeg extraction failed. Trying adjacent frames...")
            success = False
            for offset in range(1, 6):
                if extract_frame_ffmpeg(video_path, target_frame - offset, frame_save_path) or \
                   extract_frame_ffmpeg(video_path, target_frame + offset, frame_save_path):
                    success = True
                    break
            if not success:
                raise RuntimeError(f"âŒ Failed to extract frame {target_frame} using both OpenCV and FFmpeg")
        frame = cv2.imread(frame_save_path)
        os.remove(frame_save_path)

    # í”„ë ˆì„ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    # âœ… CustomCLIPVisionEncoderì˜ image_processorë¡œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    background_color = [int(c * 255) for c in image_processor.image_mean]  
    pil_image = expand2square(pil_image, tuple(background_color))
    inputs = image_processor.preprocess(pil_image, return_tensors="pt")["pixel_values"].squeeze(0).to(device)

    # í”¼ì²˜ ì¶”ì¶œ
    with torch.no_grad():
        feature = vision_tower(inputs.unsqueeze(0))  # âœ… ë°°ì¹˜ í˜•íƒœë¡œ ì…ë ¥

    return feature
