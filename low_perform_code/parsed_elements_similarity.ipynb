{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_configs import DATASETS\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from llm_prompting import filter_and_integrate\n",
    "from torchvision import transforms\n",
    "from vlm_localizer import localize\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 16x16 to 26x26\n"
     ]
    }
   ],
   "source": [
    "model, vis_processors, text_processors = load_model_and_preprocess(\"blip2_image_text_matching\", \"coco\", device='cuda', is_eval=True)\n",
    "vis_processors = transforms.Compose([\n",
    "    t for t in vis_processors['eval'].transform.transforms if not isinstance(t, transforms.ToTensor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(candidates, gt):\n",
    "    start, end = candidates[:,0], candidates[:,1]\n",
    "    s, e = gt[0], gt[1]\n",
    "    inter = np.minimum(end, e) - np.maximum(start, s)\n",
    "    union = np.maximum(end, e) - np.minimum(start, s)\n",
    "    return inter.clip(min=0) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_proposal(inputs, gamma=0.6):\n",
    "    weights = inputs[:, -1].clip(min=0)\n",
    "    proposals = inputs[:, :-1]\n",
    "    scores = np.zeros_like(weights)\n",
    "\n",
    "    for j in range(scores.shape[0]):\n",
    "        iou = calc_iou(proposals, proposals[j])\n",
    "        scores[j] += (iou ** gamma * weights).sum()\n",
    "\n",
    "    idx = np.argsort(-scores)\n",
    "    return inputs[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Evaluation for training-free video temporal grounding.')\n",
    "    parser.add_argument('--dataset', default='charades', type=str, help='Specify the dataset. See supported datasets in data_configs.py.')\n",
    "    parser.add_argument('--split', default='default', type=str, help='Specify the split. See supported splits in data_configs.py.')\n",
    "    parser.add_argument('--llm_output', default=None, type=str, help='LLM prompt output. If not specified, use only VLM for evaluation.')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(video_features, sentences):\n",
    "    with torch.no_grad():\n",
    "        text = model.tokenizer(sentences, padding='max_length', truncation=True, max_length=35, return_tensors=\"pt\").to('cuda')                    \n",
    "        text_output = model.Qformer.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True)\n",
    "        text_feat = model.text_proj(text_output.last_hidden_state[:,0,:])\n",
    "    \n",
    "    v1 = F.normalize(text_feat, dim=-1)\n",
    "    v2 = F.normalize(torch.tensor(video_features, device='cuda', dtype=v1.dtype), dim=-1)\n",
    "    # 텍스트와 비디오 특징 간의 내적(유사도) 계산\n",
    "    scores = torch.einsum('md,npd->mnp', v1, v2)\n",
    "    scores, _ = scores.max(dim=-1)\n",
    "    scores = scores.mean(dim=0, keepdim=True)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_llm(data, feature_path, stride, max_stride_factor, pad_sec=0.0):\n",
    "    ious = []\n",
    "    thresh = np.array([0.3, 0.5, 0.7])\n",
    "    recall = np.array([0, 0, 0])\n",
    "\n",
    "    pbar = tqdm(data.items())\n",
    "    for vid, ann in pbar:\n",
    "        duration = ann['duration']\n",
    "        video_feature = np.load(os.path.join(feature_path, vid+'.npy'))\n",
    "\n",
    "        for i in range(len(ann['sentences'])):\n",
    "            # sub queries\n",
    "            sub_query_proposals = []\n",
    "            if 'query_json' in ann['response'][i]:\n",
    "                relation = ann['response'][i]['relationship']\n",
    "                # j의 range가 1부터 시작하는 이유는 0번째는 sub-query가 아닌 전체 query이기 때문\n",
    "                for j in range(1, len(ann['response'][i]['query_json'])):\n",
    "                    query_json = [{'descriptions': q} for q in ann['response'][i]['query_json'][j]['descriptions']]\n",
    "                    # 하나의 description에 대해 10개 이하의 response(st:end, confidence) / 10개 이하인 이유는 10개를 뽑지만 nms에 의해 억제된 경우 그 이하의 proposal들이 반환되기 때문\n",
    "                    answers = localize(video_feature, duration, query_json, stride, int(video_feature.shape[0] * max_stride_factor))\n",
    "                    proposals = []\n",
    "                    proposal_to_description_map = []\n",
    "                    \n",
    "                    # 각 description에 대한 response에서 상위 3개만 proposal에 저장 -> proposals에는 총 9개의 구간 저장\n",
    "                    for t in range(3):\n",
    "                        for idx, p in enumerate(answers):\n",
    "                            if len(p['response']) > t:\n",
    "                                proposals.append([p['response'][t]['start'], p['response'][t]['end'], p['response'][t]['confidence']])\n",
    "                                proposal_to_description_map.append(query_json[idx]['descriptions']) \n",
    "                    \n",
    "                    proposals = np.array(proposals)\n",
    "                    proposals, selected_idx = select_proposal(np.array(proposals))\n",
    "                    # 하나의 sub-query에 대해서 3개의 proposal을 선택\n",
    "                    sub_query_proposals.append(proposals[:3])\n",
    "                    selected_idx = selected_idx[:3]\n",
    "                    \n",
    "                    selected_description = [proposal_to_description_map[idx] for idx in selected_idx[:3]]\n",
    "\n",
    "            else:\n",
    "                relation = 'single-query'\n",
    "\n",
    "            # query, 원문 쿼리 하나 + llm 생성 description 3개 => 4개의 description\n",
    "            query_json = [{'descriptions': ann['sentences'][i]}]\n",
    "            if 'query_json' in ann['response'][i]:\n",
    "                query_json += [{'descriptions': q} for q in ann['response'][i]['query_json'][0]['descriptions']]\n",
    "            answers = localize(video_feature, duration, query_json, stride, int(video_feature.shape[0] * max_stride_factor))\n",
    "            proposals = []\n",
    "            proposal_to_description_map = []  # description 인덱스를 추적하기 위한 리스트\n",
    "            \n",
    "            for t in range(3):\n",
    "                for idx, p in enumerate(answers):\n",
    "                    if len(p['response']) > t:\n",
    "                        proposals.append([p['response'][t]['start'], p['response'][t]['end'], p['response'][t]['confidence']])\n",
    "                        proposal_to_description_map.append(query_json[idx]['descriptions'])  # 해당 proposal의 description 저장\n",
    "\n",
    "            print(sub_query_proposals)\n",
    "            integrated_sub_query_proposals, index = filter_and_integrate(sub_query_proposals, relation)\n",
    "            print(integrated_sub_query_proposals)\n",
    "            print(index)\n",
    "            print('\\n')\n",
    "            # 총 12개의 proposals에서 앞 7개의 proposals 가져옴 -> 각 description 별 1개씩 + 3개\n",
    "            # proposals = proposals[:7]\n",
    "            # proposal_to_description_map = proposal_to_description_map[:7]\n",
    "\n",
    "            # proposals, selected_idx = select_proposal(np.array(proposals))\n",
    "\n",
    "            # # 가장 높은 score로 선택된 proposal에 해당하는 description을 query_json에서 찾음\n",
    "            # selected_description = proposal_to_description_map[selected_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4885 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.        , 21.21154022,  1.        ],\n",
      "       [ 0.        , 21.21154022,  1.        ],\n",
      "       [ 0.        , 21.21154022,  1.        ]]), array([[ 0.        , 21.21154022,  1.        ],\n",
      "       [ 0.        , 21.21154022,  1.        ],\n",
      "       [ 0.        , 21.21154022,  1.        ]])]\n",
      "[0.0, 21.21154022216797, 1.0]\n",
      "[0.0, 21.21154022216797, 1.0]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4885 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.        , 22.80240631,  1.        ],\n",
      "       [ 0.        , 22.80240631,  1.        ],\n",
      "       [ 0.        , 22.27211761,  1.        ]]), array([[ 2.65144231, 49.84711838,  1.        ],\n",
      "       [ 2.65144231, 50.37740707,  1.        ],\n",
      "       [ 5.30288462, 49.31682968,  1.        ]]), array([[ 0.        , 21.74182892,  1.        ],\n",
      "       [ 0.        , 21.74182892,  1.        ],\n",
      "       [ 0.        , 22.80240631,  1.        ]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/activitynet/llm_outputs-parsed_query.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 3\u001b[0m \u001b[43meval_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/ActivityNet/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 57\u001b[0m, in \u001b[0;36meval_with_llm\u001b[0;34m(data, feature_path, stride, max_stride_factor, pad_sec)\u001b[0m\n\u001b[1;32m     54\u001b[0m             proposal_to_description_map\u001b[38;5;241m.\u001b[39mappend(query_json[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptions\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# 해당 proposal의 description 저장\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(sub_query_proposals)\n\u001b[0;32m---> 57\u001b[0m integrated_sub_query_proposals, index \u001b[38;5;241m=\u001b[39m filter_and_integrate(sub_query_proposals, relation)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(integrated_sub_query_proposals)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(index)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "with open('./dataset/activitynet/llm_outputs-parsed_query.json') as f:\n",
    "    data = json.load(f)\n",
    "eval_with_llm(data, './datasets/ActivityNet/',40, 1, 0.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFVTG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
